{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import time\n",
    "import urllib\n",
    "import urllib.request as urllib2\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求头，模拟浏览器请求\n",
    "hds=[{'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'},\\\n",
    "{'User-Agent':'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'},\\\n",
    "{'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于电影详细页的数据，多为原始文本信息，没有标签，因此转成列表的形式做处理\n",
    "# 本函数从列表中提取query_text提示的信息\n",
    "def getctx(list, query_text):\n",
    "    # 查询信息在列表中的位置\n",
    "    idx = list.index(query_text) + 1\n",
    "    string = \"\"\n",
    "    # \\n表示结束，在结束之前所有信息拼接到string中\n",
    "    while list[idx] != '\\n':\n",
    "        string += list[idx] + \" \"\n",
    "        idx += 1\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对列表进行预处理，过滤列表中的无用字符\n",
    "def preprocess(list):\n",
    "    while ': ' in list:\n",
    "        list.remove(': ')\n",
    "    while ' / ' in list:\n",
    "        list.remove(' / ')\n",
    "    while ' ' in list:\n",
    "        list.remove(' ')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据保存至excel文件\n",
    "def print_excel(list):\n",
    "    wb=Workbook()\n",
    "    ws=wb.active\n",
    "    ws.title = \"豆瓣电影榜单\"\n",
    "    ws.append(['电影名','评分','导演','类型','地区','语言', '上映时间'])\n",
    "    count=1\n",
    "    for item in list:\n",
    "        ws.append(item)\n",
    "        count+=1\n",
    "    save_path = '豆瓣电影榜单.xlsx'\n",
    "    wb.save(save_path)\n",
    "\n",
    "# 将数据保存至csv文件\n",
    "def print_csv(list):\n",
    "    header = ['电影名','评分','导演','类型','地区','语言', '上映时间']\n",
    "\n",
    "    with open('豆瓣电影榜单.csv','w')as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        f_csv.writerow(header)\n",
    "        f_csv.writerows(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬虫主函数\n",
    "def movie_spider():\n",
    "    page_num=0; # 页数\n",
    "    movie_list=[] # 爬取的电影信息\n",
    "    try_times=0 # 尝试次数\n",
    "    \n",
    "    while(True):\n",
    "        # 形成每页的url\n",
    "        url='https://movie.douban.com/top250?start=' + str(page_num * 25) + '&filter='\n",
    "        # 防止请求太多频繁导致ip封禁\n",
    "        time.sleep(np.random.rand()*5)\n",
    "        \n",
    "        # 获取url下的信息\n",
    "        try:\n",
    "            req = urllib2.Request(url, headers=hds[page_num%len(hds)])\n",
    "            source_code = urllib2.urlopen(req).read().decode()\n",
    "            plain_text=str(source_code)\n",
    "        except (urllib2.HTTPError, urllib2.URLError) as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # 查找指定标签下的数据\n",
    "        soup = BeautifulSoup(plain_text)\n",
    "        list_soup = soup.findAll('div', {'class': 'hd'})\n",
    "        \n",
    "        # 对访问情况进行管理控制\n",
    "        try_times+=1;\n",
    "        if list_soup==None and try_times<200:\n",
    "            continue\n",
    "        elif list_soup==None or len(list_soup)<=1:\n",
    "            break\n",
    "            \n",
    "        # 对每一项查到的数据，获取其中包含的超链接，再次进行访问，再对第二个页面进行解析\n",
    "        for movie_info in list_soup:\n",
    "            # 找到该电影详细信息的url，获取详细信息\n",
    "            url2 = movie_info.find('a').attrs['href']\n",
    "            detailreq = urllib2.Request(url2, headers=hds[page_num%len(hds)])\n",
    "            detail = urllib2.urlopen(detailreq).read().decode()\n",
    "            detail_text=str(detail)\n",
    "            detail_soup = BeautifulSoup(detail_text)\n",
    "\n",
    "            # 找到需要信息的所在位置\n",
    "            content = detail_soup.find('div', {'id': 'content'})\n",
    "            info = content.find('div', {'id': 'info'})\n",
    "            # 由于信息是纯文本信息，没有标签可以查找，整理成list做处理\n",
    "            infolist = info.find_all(text=True)\n",
    "            infolist = preprocess(infolist)\n",
    "            rate = content.find('div', {'class': 'rating_self clearfix'})\n",
    "            \n",
    "            try:\n",
    "                title = content.h1.span.string.strip() # 标题\n",
    "            except:\n",
    "                title = \"暂无\"\n",
    "            try: \n",
    "                director = info.find('span', {'class': 'attrs'}).string.strip() # 导演\n",
    "            except:\n",
    "                director = \"暂无\"\n",
    "            try:\n",
    "                type = getctx(infolist, '类型:') # 类型\n",
    "            except:\n",
    "                type = \"暂无\"\n",
    "            try:\n",
    "                area = getctx(infolist, '制片国家/地区:') # 地区\n",
    "            except:\n",
    "                area = \"暂无\"\n",
    "            try:\n",
    "                lang = getctx(infolist, '语言:') # 语言\n",
    "            except:\n",
    "                lang = \"暂无\"\n",
    "            try:\n",
    "                releasetime = getctx(infolist, '上映日期:') # 上映日期\n",
    "            except:\n",
    "                releasetime = \"暂无\"\n",
    "            try:\n",
    "                rate = rate.strong.string # 评分\n",
    "            except:\n",
    "                rate = \"暂无\"\n",
    "                \n",
    "            # 添加到列表\n",
    "            movie_list.append([title, rate, director, type, area, lang, releasetime])\n",
    "            try_times=0\n",
    "        \n",
    "        page_num+=1 # 下一页\n",
    "        if page_num >= 6:\n",
    "            break\n",
    "        # 提示信息\n",
    "        print('Downloading Information From Page %d' % page_num)\n",
    "    return movie_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-66ed04cb4c2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmovie_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovie_spider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-117-9dcf30f12643>\u001b[0m in \u001b[0;36mmovie_spider\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'https://movie.douban.com/top250?start='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_num\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'&filter='\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 启动爬虫\n",
    "movie_list = movie_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据形成csv\n",
    "print_csv(movie_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda6405debc3ceb422f862cc4a24a3e3d81"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
